# import csv
# import os
# from urllib.parse import urljoin
#
# import django
# import unidecode
#
# from main_parcer.scripts_parcers.categories import CATEGORIES
#
# os.environ.setdefault("DJANGO_SETTINGS_MODULE", "goabay_bot.settings")
# django.setup()
#
# import requests
# import json
# from bs4 import BeautifulSoup
# import re
# from django.utils.text import slugify
# from bot_app.models import Product, ProductImage  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
# from site_app.models import Brand, Category
# import logging
#
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
#
# MEDIA_ROOT = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '../media')
# BASE_URL = "https://in.puma.com"
#
# def generate_unique_slug(base_slug):
#     slug = base_slug
#     counter = 1
#     while Product.objects.filter(slug=slug).exists():
#         slug = f"{base_slug}-{counter}"
#         counter += 1
#     return slug
#
# def sanitize_filename(filename):
#     filename = filename.replace('‚Äô', "'")
#     filename = re.sub(r'[^\x00-\x7F]+', '', filename)  # —É–¥–∞–ª—è–µ–º –≤—Å–µ –Ω–µ-ASCII —Å–∏–º–≤–æ–ª—ã
#     filename = re.sub(r'[^\w\-_\.]', '', filename)     # —É–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
#     return filename
#
# def download_image(url, product_name, is_additional=False):
#     try:
#         response = requests.get(url, stream=True, timeout=15)
#         response.raise_for_status()
#
#         # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
#         original_filename = os.path.basename(url.split("?")[0])
#         name_part, ext = os.path.splitext(original_filename)
#
#         ext = ext if ext.lower() in ['.jpg', '.jpeg', '.png', '.webp'] else '.jpg'
#
#         # –°–ª–∞–≥ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞
#         product_slug = slugify(product_name)[:40]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
#         filename_clean = sanitize_filename(name_part)[:80]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
#
#         # –¢–æ–ª—å–∫–æ –æ–¥–Ω–∞ —á–∞—Å—Ç—å –∏–º–µ–Ω–∏ (–ª–∏–±–æ slug, –ª–∏–±–æ clean filename)
#         final_filename = f"{product_slug}{ext}"  # –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ slug –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
#
#         # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
#         subfolder = 'products/additional' if is_additional else 'products'
#         media_dir = os.path.join(MEDIA_ROOT, subfolder)
#         filepath = os.path.join(subfolder, final_filename)
#         full_path = os.path.join(MEDIA_ROOT, filepath)
#
#         os.makedirs(media_dir, exist_ok=True)
#
#         # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
#         with open(full_path, 'wb') as f:
#             for chunk in response.iter_content(1024):
#                 f.write(chunk)
#
#         logging.info(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath}")
#         return filepath
#
#     except requests.exceptions.RequestException as e:
#         logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å {url}: {e}")
#         return None
#     except Exception as e:
#         logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å {url}: {e}")
#         return None
#
# def collect_product_links_from_category(category_url):
#     """
#     –°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã –∏–∑ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
#
#     –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
#         category_url (str): URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
#
#     –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
#         list: –°–ø–∏—Å–æ–∫ –ø–æ–ª–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã.
#     """
#     try:
#         response = requests.get(category_url, timeout=10)
#         response.raise_for_status()
#         soup = BeautifulSoup(response.content, 'html.parser')
#         product_links = set()
#
#         # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ <li> —Å data-test-id="product-list-item"
#         product_items = soup.find_all('li', attrs={'data-test-id': 'product-list-item'})
#
#         for item in product_items:
#             a_tag = item.find('a', attrs={'data-test-id': 'product-list-item-link'}, href=True)
#             if a_tag:
#                 # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL –∏–∑ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ href
#                 full_url = urljoin(BASE_URL, a_tag['href'])
#                 product_links.add(full_url)
#
#         return list(product_links)
#
#     except requests.exceptions.RequestException as e:
#         logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {category_url}: {e}")
#         return []
#     except Exception as e:
#         logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {category_url}: {e}")
#         return []
#
# def parse_puma_product(html_content, product_url):
#     """
#     –ü–∞—Ä—Å–∏—Ç HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–¥—É–∫—Ç–∞.
#
#     –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
#         html_content (str): HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–¥—É–∫—Ç–∞.
#         product_url (str): URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–¥—É–∫—Ç–∞.
#
#     –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
#         tuple: –ö–æ—Ä—Ç–µ–∂ —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ–¥—É–∫—Ç–∞, –Ω–∞–∑–≤–∞–Ω–∏–µ–º, —Ü–µ–Ω–æ–π –∏ –æ–ø–∏—Å–∞–Ω–∏–µ–º.
#     """
#     try:
#         soup = BeautifulSoup(html_content, 'html.parser')
#
#         data = {}
#
#         import unicodedata
#
#         # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ç–æ–≤–∞—Ä–∞ –ø–æ –Ω–æ–≤–æ–º—É —Å–µ–ª–µ–∫—Ç–æ—Ä—É
#         title_element = soup.find('h1', attrs={'data-test-id': 'pdp-title'})
#         raw_name = title_element.text.strip() if title_element else "–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
#
#         def is_letter_digit_or_space(char):
#             """
#             –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–∏–º–≤–æ–ª –±—É–∫–≤–æ–π (–ª—é–±–æ–π –∞–ª—Ñ–∞–≤–∏—Ç Unicode),
#             —Ü–∏—Ñ—Ä–æ–π –∏–ª–∏ –ø—Ä–æ–±–µ–ª–æ–º.
#             """
#             uni_category = unicodedata.category(char)
#             return uni_category.startswith('L') or uni_category.startswith('N') or char.isspace()
#
#         # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã
#         name = ''.join(char for char in raw_name if is_letter_digit_or_space(char))
#
#         # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã (–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥—Ä—è–¥)
#         name = ' '.join(name.split())
#
#         data['Name'] = name
#
#         # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Ç–æ–≤–∞—Ä
#         data['Product urls'] = product_url
#
#         # –ü–æ–ª—É—á–∞–µ–º –±–ª–æ–∫ —Å —Ü–µ–Ω–æ–π –ø–æ –Ω–æ–≤–æ–º—É —Å–µ–ª–µ–∫—Ç–æ—Ä—É
#         price = None
#         price_region = soup.find('div', attrs={'data-test-id': 'pdp-price-region'})
#         if price_region:
#             # –ò—â–µ–º —Ü–µ–Ω—É —Å–æ —Å–∫–∏–¥–∫–æ–π (–∞–∫—Ç—É–∞–ª—å–Ω—É—é —Ü–µ–Ω—É)
#             sale_price_span = price_region.find('span', attrs={'data-test-id': 'item-sale-price-pdp'})
#             if sale_price_span and sale_price_span.text.strip():
#                 price_str = sale_price_span.text.strip()
#             else:
#                 # –ï—Å–ª–∏ –Ω–µ—Ç —Ü–µ–Ω—ã —Å–æ —Å–∫–∏–¥–∫–æ–π, –±–µ—Ä–µ–º –æ–±—ã—á–Ω—É—é —Ü–µ–Ω—É
#                 price_span = price_region.find('span', attrs={'data-test-id': 'item-price-pdp'})
#                 price_str = price_span.text.strip() if price_span else None
#
#             if price_str:
#                 # –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã –≤–∞–ª—é—Ç—ã –∏ –ø—Ä–æ–±–µ–ª—ã
#                 price_str_clean = price_str.replace('‚Çπ', '').replace(',', '').strip()
#                 try:
#                     price = float(re.sub(r'[^\d.]', '', price_str_clean))
#                 except ValueError:
#                     logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ü–µ–Ω—É –≤ —á–∏—Å–ª–æ: {price_str_clean}")
#                     price = None
#
#         data['price'] = price if price is not None else 0.0
#
#         # –û–ø–∏—Å–∞–Ω–∏–µ
#         description = ""
#         product_details_div = soup.find("div", attrs={"data-test-id": "pdp-product-description"})
#
#         if product_details_div:
#             # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ "Description" (–≤ –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ —ç—Ç–æ <h2> —Å —Ç–µ–∫—Å—Ç–æ–º "Description")
#             description_h2 = product_details_div.find(
#                 lambda tag: tag.name in ['h2', 'h3'] and tag.get_text(strip=True) == "Description")
#             if description_h2:
#                 # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π sibling —Å —Ç–µ–∫—Å—Ç–æ–º –æ–ø–∏—Å–∞–Ω–∏—è (–æ–±—ã—á–Ω–æ div —Å —Ç–µ–∫—Å—Ç–æ–º)
#                 description_div = description_h2.find_next_sibling()
#                 if description_div:
#                     description_text = description_div.get_text(separator="\n", strip=True)
#                     if description_text:
#                         description += description_text + "\n\n"
#
#             # –ò—â–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, ul —Å –∫–ª–∞—Å—Å–æ–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º 'list-disc')
#             additional_lists = product_details_div.find_all('ul', class_=lambda x: x and 'list-disc' in x)
#             for ul in additional_lists:
#                 ul_texts = []
#                 for li in ul.find_all('li'):
#                     li_text = li.get_text(strip=True)
#                     if li_text:
#                         ul_texts.append(f"- {li_text}")
#                 if ul_texts:
#                     description += "Additional Information:\n" + "\n".join(ul_texts) + "\n\n"
#
#         data['descriptions'] = description.strip()
#
#         # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ additional_description
#         additional_description = ""
#         product_options_wrapper = soup.find('div', class_='product-options-wrapper')
#         if product_options_wrapper:
#             customize_title = product_options_wrapper.find('span', id='customizeTitle')
#             if customize_title:
#                 additional_description += f"Customize: {customize_title.text.strip()}\n"
#
#             bundle_options = product_options_wrapper.find_all('div', class_='field choice')
#             for option in bundle_options:
#                 label = option.find('label', class_='label')
#                 if label:
#                     additional_description += f"- {label.text.strip()}\n"
#
#         data['additional_description'] = additional_description.strip()
#         desc = data['descriptions']
#
#         # Images
#         image_urls = []
#         gallery_section = soup.find('div', class_='tw-vk1bow')
#         if gallery_section:
#             # –ò—â–µ–º –≤—Å–µ <img> –≤–Ω—É—Ç—Ä–∏ —Å–µ–∫—Ü–∏–∏ –≥–∞–ª–µ—Ä–µ–∏
#             img_tags = gallery_section.find_all('img')
#             for img in img_tags:
#                 # –ë–µ—Ä—ë–º src —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å http (–∏–ª–∏ https)
#                 src = img.get('src')
#                 if src and src.startswith('http'):
#                     image_urls.append(src)
#         data['image_urls'] = image_urls  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
#
#         # –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
#         breadcrumbs_div = soup.find('div', class_='breadcrumbs')
#         if breadcrumbs_div:
#             breadcrumb_links = breadcrumbs_div.find_all('a', class_='arv')
#             categories = [link.text.strip() for link in breadcrumb_links]
#             data['Category'] = categories[0] if categories else "–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
#             data['Subcategory'] = categories[1:] if len(categories) > 1 else []
#         else:
#             data['Category'] = "–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
#             data['Subcategory'] = []
#
#         # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å
#         data['Brand'] = "Puma"
#
#         # –†–∞–∑–º–µ—Ä
#         data['Sizes'] = "100 –∫–∞–ø—Å—É–ª"
#
#         # –î–æ—Å—Ç–∞–≤–∫–∞
#         shipping_info = []
#         shipping_wrap = soup.find('div', class_='shipping-wrap')
#         if shipping_wrap:
#             shipping_blocks = shipping_wrap.find_all('div', class_='shipping-block')
#             for block in shipping_blocks:
#                 text = block.find('p').text.strip()
#                 shipping_info.append(text)
#         data['Delivery'] = shipping_info if shipping_info else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ—Å—Ç–∞–≤–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
#
#         # SKU
#         sku = None
#         more_info_table = soup.find('table', class_='data table additional-attributes')
#         if more_info_table:
#             sku_row = more_info_table.find('tr')
#             if sku_row:
#                 sku_label_cell = sku_row.find('td', class_='col label sku')
#                 sku_data_cell = sku_row.find('td', class_='col data sku')
#                 if sku_label_cell and sku_data_cell:
#                     if sku_label_cell.text.strip() == 'SKU:':
#                         sku = sku_data_cell.text.strip()
#
#         data['SKU'] = sku if sku else "SKU –Ω–µ –Ω–∞–π–¥–µ–Ω"
#
#         # –†–µ–π—Ç–∏–Ω–≥
#         rating_summary = soup.find('div', class_='rating-summary')
#         if rating_summary:
#             rating_result = rating_summary.find('div', class_='rating-result')
#             if rating_result:
#                 title = rating_result['title']
#                 # Extract the percentage from the title
#                 rating_percentage = title.replace('%', '').strip()
#                 try:
#                     rating = float(rating_percentage)
#                 except ValueError:
#                     rating = 0.0
#                 data['rating'] = str(rating)
#                 print(f"Rating: {rating}")
#             else:
#                 data['rating'] = "0"
#                 print("Rating result not found")
#         else:
#             data['rating'] = "0"
#             print("Rating summary not found")
#
#         return data, name, price, desc
#
#     except Exception as e:
#         logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –ø—Ä–æ–¥—É–∫—Ç–∞ {product_url}: {e}")
#         return None, None, None, None
#
# def save_product_to_db(data, name, price, desc):
#     try:
#         logging.info(f"–ù–∞—á–∞–ª–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞: {name}")
#
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –ø–æ–ª—è Category
#         category_name = data.get('Category') or "General"
#         category, created = Category.objects.get_or_create(name=category_name)
#
#         # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –æ–±—Ä–µ–∑–∞–µ–º –¥–æ 100 —Å–∏–º–≤–æ–ª–æ–≤
#         cleaned_name = unidecode.unidecode(name)
#         cleaned_name = re.sub(r'[^a-zA-Z0-9\s]', '', cleaned_name).strip()[:100]
#
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –ø–æ–ª—è SKU
#         sku = data.get('SKU')
#         if not sku or sku.strip().lower() == 'sku –Ω–µ –Ω–∞–π–¥–µ–Ω':
#             sku = None  # –û—á–∏—Å—Ç–∫–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
#
#         # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ slug
#         base_slug = slugify(sku)[:500] if sku else slugify(cleaned_name)[:500]
#         slug = generate_unique_slug(base_slug)
#
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞
#         brand_name = data.get('Brand') or 'Puma'
#         brand, brand_created = Brand.objects.get_or_create(
#             name=brand_name,
#             defaults={'slug': slugify(brand_name)}
#         )
#
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
#         image_urls = data.get('image_urls', [])
#         main_image_url = image_urls[0] if image_urls else None
#         main_image_path = download_image(main_image_url, slug) if main_image_url else None
#
#         # –ï—Å–ª–∏ –ø—É—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –æ–±—Ä–µ–∑–∞–µ–º –µ–≥–æ
#         if main_image_path and len(main_image_path) > 100:
#             main_image_path = main_image_path[:100]
#
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞
#         existing_product = Product.objects.filter(slug=slug).first()  # –ò—â–µ–º –ø–æ slug, –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º sku
#
#         if existing_product:
#             # –ï—Å–ª–∏ –ø—Ä–æ–¥—É–∫—Ç –Ω–∞–π–¥–µ–Ω, –æ–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ü–µ–Ω—É
#             logging.info(f"–ü—Ä–æ–¥—É–∫—Ç {name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –û–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–Ω—É.")
#             existing_product.price = price
#             existing_product.save()
#         else:
#             # –ï—Å–ª–∏ –ø—Ä–æ–¥—É–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
#             logging.info(f"–°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π –ø—Ä–æ–¥—É–∫—Ç: {name}")
#             product = Product.objects.create(
#                 slug=slug,
#                 name=cleaned_name,
#                 desc=desc,
#                 price=price,
#                 image=main_image_path,
#                 rating=data.get('rating', '0'),
#                 additional_description=data.get('additional_description', ''),
#                 brand=brand,
#                 sku=sku  # –±–µ–∑–æ–ø–∞—Å–Ω–æ: None –∏–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π SKU
#             )
#
#             # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
#             product.category.add(category)
#
#         # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
#         if len(image_urls) > 1:
#             for img_url in image_urls[1:]:
#                 try:
#                     additional_image_path = download_image(img_url, slug, is_additional=True)
#                     # –ï—Å–ª–∏ –ø—É—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –æ–±—Ä–µ–∑–∞–µ–º –µ–≥–æ
#                     if additional_image_path and len(additional_image_path) > 100:
#                         additional_image_path = additional_image_path[:100]
#                     if additional_image_path:
#                         if not ProductImage.objects.filter(product=product, image=additional_image_path).exists():
#                             ProductImage.objects.create(product=product, image=additional_image_path)
#                 except Exception as e:
#                     logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
#
#         return True
#     except Exception as e:
#         logging.error(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞ {name}: {e}", exc_info=True)
#         return False
#
# def collect_all_product_links(category_urls):
#     """
#     –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–π.
#
#     –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
#         category_urls (list): –°–ø–∏—Å–æ–∫ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–π.
#
#     –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
#         list: –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã.
#     """
#     all_product_links = set()
#     for url in category_urls:
#         product_links = collect_product_links_from_category(url)
#         all_product_links.update(product_links)
#     return list(all_product_links)
#
# def create_hierarchy(data):
#     for main, subs in data.items():
#         main_cat, created_main = Category.objects.get_or_create(name=main, parent=None)
#         for sub, subsubs in subs.items():
#             sub_cat, created_sub = Category.objects.get_or_create(name=sub, parent=main_cat)
#             for subsub in subsubs:
#                 _, created_subsub = Category.objects.get_or_create(name=subsub, parent=sub_cat)
#                 if created_subsub:
#                     print(f"–°–æ–∑–¥–∞–Ω–∞ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {subsub} ‚Üí {sub} ‚Üí {main}")
#     print("–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã.")
#
# if __name__ == "__main__":
#     create_hierarchy(CATEGORIES)
#     category_urls = [
#         'https://in.puma.com/in/en/mens',
#
#     ]
#
#     product_links = collect_all_product_links(category_urls)  # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã
#     print(f"–ù–∞–π–¥–µ–Ω–æ {len(product_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã.")
#
#     all_products_data = []  # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –≤—Å–µ—Ö –ø—Ä–æ–¥—É–∫—Ç–∞—Ö
#     for product_url in product_links:
#         try:
#             response = requests.get(product_url, timeout=10)  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º GET –∑–∞–ø—Ä–æ—Å –∫ URL –ø—Ä–æ–¥—É–∫—Ç–∞
#             response.raise_for_status()  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
#
#             html_content = response.text  # –ü–æ–ª—É—á–∞–µ–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
#
#         except requests.exceptions.RequestException as e:
#             logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å {product_url}: {e}")  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –∑–∞–ø—Ä–æ—Å–∞
#             continue
#
#         product_data, name, price, desc = parse_puma_product(html_content, product_url)  # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç–∞
#         if product_data:
#             all_products_data.append(product_data)  # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç–∞ –≤ —Å–ø–∏—Å–æ–∫
#             print(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–¥—É–∫—Ç–µ —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–∞ —Å {product_url}")
#             save_product_to_db(product_data, name, price, desc)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
#         else:
#             print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–¥—É–∫—Ç–µ —Å {product_url}")
#
#     with open('jsons/product_puma.json', 'w', encoding='utf-8') as f:
#         json.dump(all_products_data, f, indent=4, ensure_ascii=False)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–∞–π–ª
#
#     print("–î–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ product_puma.json")
#
#     csv_file = 'jsons/product_puma.csv'
#
#     if all_products_data:
#         # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø–æ–ª—è —Å —Å–ø–∏—Å–∫–∞–º–∏ –≤ —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Å–∫–æ–±–æ–∫
#         for product in all_products_data:
#             # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—è —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
#             if isinstance(product.get('image_urls'), list):
#                 product['image_urls'] = ', '.join(product['image_urls'])
#             # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π (—Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã)
#             if isinstance(product.get('Subcategory'), list):
#                 product['Subcategory'] = ', '.join(product['Subcategory'])
#             # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—è –¥–æ—Å—Ç–∞–≤–∫–∏ (—Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã)
#             if isinstance(product.get('Delivery'), list):
#                 product['Delivery'] = ', '.join(product['Delivery'])
#
#         # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –∫–ª—é—á–µ–π –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
#         fieldnames = all_products_data[0].keys()
#
#         with open(csv_file, 'w', newline='', encoding='utf-8') as f_csv:
#             writer = csv.DictWriter(f_csv, fieldnames=fieldnames)
#             writer.writeheader()
#             writer.writerows(all_products_data)
#     else:
#         print("–î–∞–Ω–Ω—ã–µ –¥–ª—è CSV –ø—É—Å—Ç—ã, —Ñ–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω.")
import time
import json
import os
import django
import csv
import requests
from bs4 import BeautifulSoup
from django.core.files.base import ContentFile
from django.utils.text import slugify

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "goabay_bot.settings")
django.setup()

from bot_app.models import Product
from site_app.models import Category, Brand


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
import time
import json


def parse_json_ld(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    script_tags = soup.find_all('script', type='application/ld+json')
    products = []
    category_name = None
    brand_from_group = None
    group_url = None

    for script_tag in script_tags:
        try:
            content = script_tag.string
            if not content:
                continue
            json_data = json.loads(content)
        except Exception:
            continue

        typ = json_data.get('@type')

        if typ == 'BreadcrumbList':
            for item in json_data.get('itemListElement', []):
                if item.get('position') == 2:
                    category_name = item.get('name')

        elif typ == 'ProductGroup':
            brand_from_group = json_data.get('brand', {}).get('name')
            group_url = json_data.get('url')
            for variant in json_data.get('hasVariant', []):
                products.append({
                    'name': variant.get('name'),
                    'url': variant.get('url') or group_url,
                    'price': variant.get('offers', {}).get('price'),
                    'currency': variant.get('offers', {}).get('priceCurrency'),
                    'image': variant.get('image'),
                    'brand': brand_from_group,
                    'category': category_name,
                    'color': variant.get('color'),
                    'size': '',
                    'description': '',
                })

        elif typ == 'Product':
            products.append({
                'name': json_data.get('name'),
                'url': json_data.get('url'),
                'price': json_data.get('offers', {}).get('price'),
                'currency': json_data.get('offers', {}).get('priceCurrency'),
                'image': json_data.get('image'),
                'brand': json_data.get('brand', {}).get('name'),
                'category': category_name,
                'color': json_data.get('color'),
                'size': '',
                'description': '',
            })

    # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
    description_tag = soup.find('div', {'data-test-id': 'pdp-product-description'})
    description = description_tag.get_text(strip=True) if description_tag else "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    for product in products:
        product['description'] = description

    # –ü–∞—Ä—Å–∏–º —Ä–∞–∑–º–µ—Ä—ã
    sizes = []
    for label in soup.select('label[data-size]'):
        size_span = label.select_one('span[data-content="size-value"]')
        if size_span:
            sizes.append(size_span.get_text(strip=True))
    sizes_str = ', '.join(sizes) if sizes else '–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏'
    for product in products:
        product['size'] = sizes_str

    return products


def save_to_db(products):
    for item in products:
        if not item.get('name'):
            continue

        slug = slugify(item['name'])[:500]
        brand = None
        if item.get('brand'):
            brand_name = item['brand'].strip()
            brand_slug = slugify(brand_name)
            brand, _ = Brand.objects.get_or_create(name=brand_name, slug=brand_slug)

        categories = []
        if item.get('category'):
            cat, _ = Category.objects.get_or_create(name=item['category'].strip())
            categories.append(cat)

        product, created = Product.objects.get_or_create(slug=slug, defaults={
            'name': item['name'],
            'brand': brand,
            'desc': item.get('description', ''),
            'price': item.get('price') or 0,
            'color': item.get('color', ''),
            'sizes': item.get('size', ''),
            'stock_status': 'in_stock',
        })

        if not created:
            product.price = item.get('price') or product.price
            product.desc = item.get('description') or product.desc
            product.color = item.get('color') or product.color
            product.sizes = item.get('size') or product.sizes
            print(f"üîÑ –û–±–Ω–æ–≤–ª—ë–Ω –ø—Ä–æ–¥—É–∫—Ç: {product.name}")
        else:
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω –ø—Ä–æ–¥—É–∫—Ç: {product.name}")

        if item.get('image') and (created or not product.image):
            try:
                img_url = item['image'] if isinstance(item['image'], str) else item['image'][0]
                img_content = requests.get(img_url).content
                image_field = ContentFile(img_content, name=img_url.split('/')[-1])
                product.image.save(image_field.name, image_field)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")

        if categories:
            product.category.set(categories)

        product.save()


if __name__ == '__main__':
    category_urls = [
        'https://in.puma.com/in/en/mens',
    ]

    product_links = collect_all_product_links(category_urls)
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(product_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã.")

    all_products_data = []
    for product_url in product_links:
        try:
            response = requests.get(product_url, timeout=10)
            response.raise_for_status()
            html_content = response.text
        except requests.RequestException as e:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å {product_url}: {e}")
            continue

        products = parse_json_ld(html_content)
        if products:
            all_products_data.extend(products)
            print(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–¥—É–∫—Ç–µ —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–∞ —Å {product_url}")
        else:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–¥—É–∫—Ç–µ —Å {product_url}")

    os.makedirs('jsons', exist_ok=True)

    with open('jsons/product_puma.json', 'w', encoding='utf-8') as f:
        json.dump(all_products_data, f, indent=4, ensure_ascii=False)
    print("‚úÖ JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω: product_puma.json")

    with open('jsons/product_puma.csv', 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['name', 'url', 'price', 'currency', 'image', 'brand', 'category', 'color', 'size', 'description']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for product in all_products_data:
            if isinstance(product['image'], list):
                product['image'] = product['image'][0]
            writer.writerow(product)
    print("‚úÖ CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: product_puma.csv")

    save_to_db(all_products_data)
