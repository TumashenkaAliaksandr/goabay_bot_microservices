# # -*- coding: utf-8 -*-
# import csv
# import logging
# import os
# import json
# import re
# import time
# import django
# import requests
# from bs4 import BeautifulSoup
# from urllib.parse import urlparse
# from django.utils.text import slugify
# from django.core.files.base import ContentFile
#
# from main_parcer.scripts_parcers.categories import CATEGORIES
#
# os.environ.setdefault("DJANGO_SETTINGS_MODULE", "goabay_bot.settings")
# django.setup()
#
# from bot_app.models import Product, ProductVariant, ProductImage
# from site_app.models import Category, Brand
#
# from selenium import webdriver
# from selenium.webdriver.common.by import By
# from selenium.webdriver.common.keys import Keys
# from selenium.webdriver.chrome.options import Options
#
# BASE_URL = "https://in.puma.com"
# MAIN_CATEGORIES = [
#     'https://in.puma.com/in/en/rcb-launch',
# ]
#
# logging.basicConfig(level=logging.DEBUG)
# logger = logging.getLogger(__name__)
#
#
#
# def extract_sizes_with_selenium(url):
#     options = Options()
#     options.add_argument("--headless")
#     options.add_argument("--no-sandbox")
#     options.add_argument("--disable-dev-shm-usage")
#
#
#     driver = webdriver.Chrome(options=options)
#     driver.get(url)
#     time.sleep(3)  # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å, –µ—Å–ª–∏ –Ω–∞–¥–æ)
#
#     # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
#     html = driver.page_source
#
#     # –ò—Å–ø–æ–ª—å–∑—É–µ–º BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
#     soup = BeautifulSoup(html, 'html.parser')
#
#     # –ü—Ä–∏–º–µ—Ä: –∏—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ data-content)
#     # –ü–æ–¥—Å—Ç–∞–≤—å—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–ª–∞—Å—Å –∏–ª–∏ –∞—Ç—Ç—Ä–∏–±—É—Ç –¥–ª—è —Ä–∞–∑–º–µ—Ä–æ–≤
#     size_elements = soup.find_all('span', {'data-content': 'size-value'})
#
#     if size_elements:
#         sizes = [size.get_text(strip=True) for size in size_elements]
#         print(f"‚úÖ –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã: {sizes}")
#         driver.quit()
#         return sizes
#     else:
#         print("‚ùå –†–∞–∑–º–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
#         driver.quit()
#         return None
#
#
# def collect_product_links_from_category(category_url, scroll_pause=2, max_wait_scrolls=300):
#     options = Options()
#     options.add_argument("--headless")
#     options.add_argument("--no-sandbox")
#     options.add_argument("--disable-dev-shm-usage")
#
#     driver = webdriver.Chrome(options=options)
#     driver.set_page_load_timeout(180)
#     driver.set_script_timeout(180)
#
#     try:
#         driver.get(category_url)
#     except Exception as e:
#         print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {category_url}: {e}")
#         driver.quit()
#         return []
#
#     product_links = set()
#     last_links_count = 0
#     wait_counter = 0
#
#     while wait_counter < max_wait_scrolls:
#         try:
#             driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)
#             time.sleep(scroll_pause)
#
#             script_tags = driver.find_elements(By.XPATH, '//script[@type="application/ld+json"]')
#             for tag in script_tags:
#                 try:
#                     content = tag.get_attribute('innerHTML')
#                     data = json.loads(content)
#                     if data.get('@type') == 'ItemList':
#                         for item in data.get('itemListElement', []):
#                             url = item.get('item', {}).get('url')
#                             if url:
#                                 product_links.add(url)
#                 except:
#                     continue
#
#             if len(product_links) == last_links_count:
#                 break
#             last_links_count = len(product_links)
#             wait_counter += 1
#         except Exception as e:
#             print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–∫—Ä—É—Ç–∫–µ: {e}")
#             break
#
#     driver.quit()
#     return list(product_links)
#
#
# def extract_sizes_from_html(html):
#     soup = BeautifulSoup(html, 'html.parser')
#     # –ò—Å–ø–æ–ª—å–∑—É–µ–º map –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
#     sizes = list(map(lambda x: x.get_text(strip=True), soup.find_all('span', {'data-content': 'size-value'})))
#
#     return sizes if sizes else ['–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏']
#
#
# def parse_json_ld(html_content, main_cat, sub_cat):
#     soup = BeautifulSoup(html_content, 'html.parser')
#
#     next_data_script = soup.find('script', id='__NEXT_DATA__')
#     if next_data_script:
#         try:
#             data = json.loads(next_data_script.string)
#             next_data_products = parse_next_data_json(data, soup, main_cat, sub_cat)
#             if next_data_products:
#                 return next_data_products
#         except Exception as e:
#             print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ __NEXT_DATA__: {e}")
#
#     return parse_json_ld_fallback(soup, main_cat, sub_cat)
#
#
# def extract_variant_info_from_insider(html):
#     """
#     –ò—â–µ—Ç –≤ html —Å–∫—Ä–∏–ø—Ç —Å window.insider_object.product –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç color –∏ size.
#     –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å –∫–ª—é—á–∞–º–∏ 'color' –∏ 'size' –∏–ª–∏ –ø—É—Å—Ç–æ–π dict, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.
#     """
#     pattern = re.compile(r'window\.insider_object\.product\s*=\s*({.*?});', re.DOTALL)
#     match = pattern.search(html)
#     if not match:
#         return {}
#
#     try:
#         product_json_str = match.group(1)
#         # –ò–Ω–æ–≥–¥–∞ –≤ JSON –º–æ–≥—É—Ç –±—ã—Ç—å –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –∏–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ –æ–±—ã—á–Ω–æ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π JS-–æ–±—ä–µ–∫—Ç
#         # –î–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –Ω–∞ –¥–≤–æ–π–Ω—ã–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ:
#         # product_json_str = product_json_str.replace("'", '"')
#         product_data = json.loads(product_json_str)
#         return {
#             'color': product_data.get('color'),
#             'size': product_data.get('size')
#         }
#     except Exception as e:
#         print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ insider_object.product: {e}")
#         return {}
#
#
# def parse_next_data_json(data, soup, main_cat, sub_cat):
#     products = []
#     try:
#         product = data['props']['pageProps'].get('product', {})
#         variants = product.get('variants', [])
#         base_name = product.get("name")
#         description_tag = soup.find('div', {'data-test-id': 'pdp-product-description'})
#         description = description_tag.get_text(strip=True) if description_tag else product.get('description', '')
#         html = str(soup)
#
#         for variant in variants:
#             image = variant.get("image", {}).get("url") or None
#             price = variant.get("price", {}).get("value")
#             currency = variant.get("price", {}).get("currency")
#
#             variant_url = variant.get("url")
#             insider_info = {}
#             sizes_str = '–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏'
#             variant_html = ''
#
#             try:
#                 if variant_url:
#                     full_url = BASE_URL + variant_url if variant_url.startswith('/') else variant_url
#                     variant_resp = requests.get(full_url, timeout=10)
#                     if variant_resp.status_code == 200:
#                         variant_html = variant_resp.text
#                         insider_info = extract_variant_info_from_insider(variant_html)
#                         sizes_list = extract_sizes_from_html(variant_html)
#                         sizes_str = ', '.join(sizes_list)
#                 else:
#                     fallback_url = product.get("url") or product.get("pdpUrl") or ""
#                     if fallback_url:
#                         if not fallback_url.startswith("http"):
#                             fallback_url = BASE_URL + fallback_url
#                         sizes_list = extract_sizes_with_selenium(fallback_url)
#                         sizes_str = ', '.join(sizes_list)
#             except Exception as e:
#                 print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ –≤–∞—Ä–∏–∞—Ü–∏–∏: {e}")
#
#             color = variant.get("color") or insider_info.get('color')
#             size = sizes_str
#
#             products.append({
#                 "base_name": base_name,
#                 "variant_name": variant.get("name"),
#                 "url": BASE_URL + variant_url if variant_url else None,
#                 "price": price,
#                 "currency": currency,
#                 "image": image,
#                 "brand": product.get("brand", {}).get("name", "PUMA"),
#                 "category": main_cat,
#                 "subcategory": sub_cat,
#                 "color": color,
#                 "size": size,
#                 "description": description,
#                 "html": variant_html
#             })
#
#     except Exception as e:
#         print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞–∑–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ __NEXT_DATA__: {e}")
#
#     return products
#
#
# def parse_json_ld_fallback(soup, main_cat, sub_cat):
#     script_tags = soup.find_all('script', type='application/ld+json')
#     description_tag = soup.find('div', {'data-test-id': 'pdp-product-description'})
#     description = description_tag.get_text(strip=True) if description_tag else "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
#
#     products = []
#     for tag in script_tags:
#         try:
#             data = json.loads(tag.string)
#         except:
#             continue
#
#         if data.get('@type') == 'ProductGroup':
#             brand = data.get('brand', {}).get('name')
#             product_name = data.get('name')
#
#             for var in data.get('hasVariant', []):
#                 variant_url = BASE_URL + var.get('url', '') if var.get('url') else None
#                 sizes_str = '–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏'
#                 html = ''
#
#                 if variant_url:
#                     try:
#                         resp = requests.get(variant_url, timeout=10)
#                         if resp.status_code == 200:
#                             html = resp.text
#                             sizes_str = extract_sizes_from_html(html)
#                     except:
#                         pass
#
#                 products.append({
#                     'base_name': product_name,
#                     'variant_name': var.get('name'),
#                     'url': variant_url,
#                     'price': var.get('offers', {}).get('price'),
#                     'currency': var.get('offers', {}).get('priceCurrency'),
#                     'image': var.get('image'),
#                     'brand': brand,
#                     'category': main_cat,
#                     'subcategory': sub_cat,
#                     'color': var.get('color'),
#                     'size': sizes_str,
#                     'description': description,
#                     'html': html
#                 })
#
#     return products
#
#
# def find_category_path(main_cat, sub_cat):
#     for top, subs in CATEGORIES.items():
#         for mid, leafs in subs.items():
#             for leaf in leafs:
#                 if leaf.lower() in sub_cat.lower():
#                     return [top, mid, leaf]
#             if sub_cat.lower() in mid.lower():
#                 return [top, mid]
#         if sub_cat.lower() in top.lower():
#             return [top]
#     return [main_cat, sub_cat] if main_cat and sub_cat else [main_cat or sub_cat]
#
#
# def get_or_create_category_from_path(path):
#     parent = None
#     for name in path:
#         if not name:
#             continue
#         obj, _ = Category.objects.get_or_create(name=name, parent=parent)
#         parent = obj
#     return parent
#
#
# def save_to_db(products):
#     grouped = {}
#     for item in products:
#         key = slugify(item['base_name'])
#         grouped.setdefault(key, []).append(item)
#
#     json_data = []
#     csv_rows = []
#
#     for slug, variants in grouped.items():
#         base = variants[0]
#
#         if base.get('html') and (not base.get('size') or base.get('size') == '–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏'):
#             base['size'] = extract_sizes_from_html(base['html'])
#
#         brand = None
#         if base.get('brand'):
#             brand_slug = slugify(base['brand'])
#             brand, _ = Brand.objects.get_or_create(name=base['brand'], slug=brand_slug)
#
#         cat_path = find_category_path(base['category'], base['subcategory'])
#         cat_obj = get_or_create_category_from_path(cat_path)
#
#         product, created = Product.objects.get_or_create(slug=slug, defaults={
#             'name': base['base_name'],
#             'brand': brand,
#             'desc': base.get('description', ''),
#             'price': base.get('price') or 0,
#             'color': base.get('color', ''),
#             'sizes': base.get('size', ''),
#             'stock_status': 'in_stock'
#         })
#
#         if not created:
#             product.name = base['base_name']
#             product.brand = brand
#             product.desc = base.get('description', '')
#             product.price = base.get('price') or 0
#             product.color = base.get('color', '')
#             product.sizes = base.get('size', '')
#             product.stock_status = 'in_stock'
#             product.save()
#
#         if cat_obj:
#             product.category.set([cat_obj])
#
#         main_image_url = base.get('image')
#         if main_image_url:
#             if isinstance(main_image_url, list):
#                 main_image_url = main_image_url[0]
#             try:
#                 img_data = requests.get(main_image_url).content
#                 product.image.save(main_image_url.split("/")[-1], ContentFile(img_data), save=True)
#             except Exception as e:
#                 print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥–ª–∞–≤–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
#
#         for img_url in base.get('additional_images', []):
#             try:
#                 img_data = requests.get(img_url).content
#                 ProductImage.objects.create(
#                     product=product,
#                     image=ContentFile(img_data, name=img_url.split("/")[-1])
#                 )
#             except Exception as e:
#                 print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
#
#         for var in variants:
#             if var.get('html') and (not var.get('size') or var.get('size') == '–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏'):
#                 var['size'] = extract_sizes_from_html(var['html'])
#
#             sku = slugify(var['variant_name'])[:100]
#             variant_obj, created = ProductVariant.objects.get_or_create(
#                 sku=sku,
#                 defaults={
#                     'product': product,
#                     'color': var.get('color'),
#                     'size': var.get('size')[:50],
#                     'price': var.get('price') or 0,
#                     'quantity': 0,
#                     'description': var.get('description', '')
#                 }
#             )
#
#             if not created:
#                 variant_obj.color = var.get('color')
#                 variant_obj.size = var.get('size')[:50]
#                 variant_obj.price = var.get('price') or 0
#                 variant_obj.description = var.get('description', '')
#                 variant_obj.save()
#
#             image_url = var.get('image')
#             if image_url:
#                 if isinstance(image_url, list):
#                     image_url = image_url[0]
#                 try:
#                     img_data = requests.get(image_url).content
#                     variant_obj.image.save(image_url.split("/")[-1], ContentFile(img_data), save=True)
#                 except Exception as e:
#                     print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–∞—Ä–∏–∞—Ü–∏–∏: {e}")
#
#             csv_rows.append({
#                 'product_name': product.name,
#                 'brand': brand.name if brand else '',
#                 'price': var.get('price') or 0,
#                 'category': cat_obj.name if cat_obj else '',
#                 'color': var.get('color'),
#                 'size': var.get('size'),
#                 'sku': sku,
#                 'description': var.get('description', ''),
#                 'image': image_url
#             })
#
#         json_data.append({
#             'name': product.name,
#             'slug': product.slug,
#             'brand': brand.name if brand else '',
#             'desc': product.desc,
#             'price': product.price,
#             'color': product.color,
#             'sizes': product.sizes,
#             'image': main_image_url,
#             'category': cat_path,
#             'variants': variants
#         })
#
#         print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω –ø—Ä–æ–¥—É–∫—Ç: {product.name}")
#
#     os.makedirs('jsons', exist_ok=True)
#
#     with open('jsons/products_data.json', 'w', encoding='utf-8') as jf:
#         json.dump(json_data, jf, ensure_ascii=False, indent=4)
#
#     with open('jsons/products_data.csv', 'w', newline='', encoding='utf-8') as cf:
#         fieldnames = ['product_name', 'brand', 'price', 'category', 'color', 'size', 'sku', 'description', 'image']
#         writer = csv.DictWriter(cf, fieldnames=fieldnames)
#         writer.writeheader()
#         writer.writerows(csv_rows)
#
#     print("üì¶ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON –∏ CSV")
#
#
# if __name__ == '__main__':
#     all_products = []
#
#     for category_url in MAIN_CATEGORIES:
#         print(f"\n‚ñ∂ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")
#         links = collect_product_links_from_category(category_url)
#         print(f"  ‚û§ –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(links)}")
#
#         parsed = urlparse(category_url).path.strip('/').split('/')
#         main_cat = parsed[2] if len(parsed) > 2 else 'unknown'
#         sub_cat = parsed[-1]
#
#         for url in links:
#             try:
#                 r = requests.get(url, timeout=30)
#                 r.raise_for_status()
#                 products = parse_json_ld(r.text, main_cat, sub_cat)
#                 all_products.extend(products)
#                 print(f"  ‚ûï {url} ({len(products)} –≤–∞—Ä–∏–∞—Ü–∏–π)")
#             except Exception as e:
#                 print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
#
#     save_to_db(all_products)
#     print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_products)}")
import csv
import json
# -------------------
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# import time
# from bs4 import BeautifulSoup
#
# # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞ –±–µ–∑ UI (headless —Ä–µ–∂–∏–º)
# options = Options()
# options.add_argument("--headless")  # –î–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–∑ UI
# options.add_argument("--no-sandbox")
# options.add_argument("--disable-dev-shm-usage")
#
# # –ü—É—Ç—å –∫ –¥—Ä–∞–π–≤–µ—Ä—É Chrome (—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–æ —Å–≤–æ–µ–º—É –ø—É—Ç–∏)
# driver = webdriver.Chrome(options=options)
#
# def extract_sizes_from_page(url):
#     driver.get(url)
#     time.sleep(3)  # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å, –µ—Å–ª–∏ –Ω–∞–¥–æ)
#
#     # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
#     html = driver.page_source
#
#     # –ò—Å–ø–æ–ª—å–∑—É–µ–º BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
#     soup = BeautifulSoup(html, 'html.parser')
#
#     # –ü—Ä–∏–º–µ—Ä: –∏—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ data-content)
#     # –ü–æ–¥—Å—Ç–∞–≤—å—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–ª–∞—Å—Å –∏–ª–∏ –∞—Ç—Ç—Ä–∏–±—É—Ç –¥–ª—è —Ä–∞–∑–º–µ—Ä–æ–≤
#     size_elements = soup.find_all('span', {'data-content': 'size-value'})
#
#     if size_elements:
#         sizes = [size.get_text(strip=True) for size in size_elements]
#         print(f"‚úÖ –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã: {sizes}")
#         driver.quit()
#         return sizes
#     else:
#         print("‚ùå –†–∞–∑–º–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
#         driver.quit()
#         return None
#
# # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
# url = "https://in.puma.com/in/en/pd/court-shatter-low-sneakers/399844?size=0240&swatch=04"
# sizes = extract_sizes_from_page(url)
#
# if sizes:
#     print(f"‚úÖ –í—Å–µ —Ä–∞–∑–º–µ—Ä—ã: {sizes}")
# else:
#     print("‚õî –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ä–∞–∑–º–µ—Ä—ã.")

import csv
import json
import os
import random
import re
import string
import uuid
from urllib.parse import urlparse

import django
import requests
from django.core.files.base import ContentFile
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "goabay_bot.settings")
django.setup()

from bs4 import BeautifulSoup
from django.utils.text import slugify
from decimal import Decimal

from bot_app.models import Product, ProductImage, ProductVariant, VariantImage
from site_app.models import Category, Brand




def generate_random_id(length=8):
    chars = string.ascii_uppercase + string.digits
    return ''.join(random.choice(chars) for _ in range(length))

def extract_product_info_and_variations(url):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(options=options)
    driver.set_window_size(1920, 1080)
    wait = WebDriverWait(driver, 15)

    try:
        driver.get(url)
        wait.until(EC.presence_of_element_located((By.ID, 'pdp-product-title')))
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–≤–∞—Ä–µ
        brand = 'Puma'
        print('Brand: ', brand)
        product_name_tag = soup.find('h1', id='pdp-product-title')
        product_name = product_name_tag.get_text(strip=True) if product_name_tag else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ"

        sale_price_tag = soup.find('span', {'data-test-id': 'item-sale-price-pdp'})
        sale_price = sale_price_tag.get_text(strip=True) if sale_price_tag else None

        original_price_tag = soup.find('span', {'data-test-id': 'item-price-pdp'})
        original_price = original_price_tag.get_text(strip=True) if original_price_tag else None

        desc_block = soup.find('div', {'data-test-id': 'pdp-product-description'})
        description = ""
        if desc_block:
            text_div = desc_block.find('div', {'data-uds-child': 'text'})
            if text_div:
                description = text_div.get_text(separator="\n", strip=True)
            else:
                description = desc_block.get_text(separator="\n", strip=True)

        main_category = ""
        subcategories = ""
        breadcrumb_nav = soup.find('nav', id='breadcrumb')
        if breadcrumb_nav:
            crumbs = breadcrumb_nav.select('ul[data-uds-child="breadcrumb-list"] li a')
            categories = [crumb.get_text(strip=True) for crumb in crumbs]
            if categories and categories[0].lower() == 'home':
                categories = categories[1:]
            if categories:
                main_category = categories[0]
                if len(categories) > 1:
                    subcategories = " / ".join(categories[1:])

        wait.until(EC.presence_of_element_located((By.ID, 'style-picker')))
        color_variants = driver.find_elements(By.CSS_SELECTOR, '#style-picker label[data-test-id="color"]')

        product_random_id = generate_random_id()

        variations = []

        for idx, color_variant in enumerate(color_variants):
            try:
                color_name = color_variant.find_element(By.CSS_SELECTOR, 'span.sr-only').text.strip()
                if not color_name:
                    color_name = f"color_{idx+1}"

                driver.execute_script("arguments[0].scrollIntoView(true);", color_variant)
                wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, f'#style-picker label[data-test-id="color"]:nth-child({idx+1})')))
                try:
                    color_variant.click()
                except Exception:
                    driver.execute_script("arguments[0].click();", color_variant)

                time.sleep(2)
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'label[data-size] span[data-content="size-value"]')))

                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')

                size_elements = soup.select('label[data-size] span[data-content="size-value"]')
                sizes = [size.get_text(strip=True) for size in size_elements if size.get_text(strip=True)]
                if not sizes:
                    sizes = ['–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏']

                gallery_section = soup.find('section', {'data-test-id': 'product-image-gallery-section'})
                main_image = None
                all_images = []
                if gallery_section:
                    main_img_tag = gallery_section.find('img', {'data-test-id': 'pdp-main-image'})
                    if main_img_tag and main_img_tag.has_attr('src'):
                        main_image = main_img_tag['src']
                    for img_tag in gallery_section.find_all('img'):
                        if img_tag.has_attr('src'):
                            all_images.append(img_tag['src'])
                    all_images = list(dict.fromkeys(all_images))

                variation = {
                    'color': color_name,
                    'sizes': sizes,
                    'main_image': main_image,
                    'all_images': all_images
                }

                variations.append(variation)

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ü–≤–µ—Ç–∞ '{color_name}': {e}")

        product_info = {
            'brand': brand,
            'random_id': product_random_id,
            'product_url': url,
            'product_name': product_name,
            'main_category': main_category,
            'subcategories': subcategories,
            'sale_price': sale_price,
            'original_price': original_price,
            'description': description.replace("\n", " ").strip(),
            'product_type': 'variative' if len(variations) > 1 else 'simple',
            'variations': variations
        }

        return [product_info]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å –æ–¥–Ω–∏–º —ç–ª–µ–º–µ–Ω—Ç–æ–º ‚Äî –æ—Å–Ω–æ–≤–Ω—ã–º –ø—Ä–æ–¥—É–∫—Ç–æ–º

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –ø–æ–∏—Å–∫–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {e}")
    finally:
        driver.quit()

    return []


# –§—É–Ω–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

def variations_to_str(variations):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞—Ä–∏–∞—Ü–∏–π –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è CSV:
    color|size1,size2|main_image|img1,img2; color2|...
    """
    result = []
    for var in variations:
        color = var.get('color', '')
        sizes = ",".join(var.get('sizes', []))
        main_image = var.get('main_image', '')
        all_images = ",".join(var.get('all_images', []))
        result.append(f"{color}|{sizes}|{main_image}|{all_images}")
    return " | ".join(result)


# def variations_to_str(variations):
#     """
#     –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞—Ä–∏–∞—Ü–∏–π –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è CSV.
#     –ö–∞–∂–¥–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è ‚Äî –Ω–∞–±–æ—Ä –∫–ª—é—á:–∑–Ω–∞—á–µ–Ω–∏–µ, —Ä–∞–∑–º–µ—Ä—ã –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω—ã —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é.
#     –í–∞—Ä–∏–∞—Ü–∏–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã ' | '.
#     """
#     if not variations:
#         return ''
#
#     parts = []
#     for var in variations:
#         color = var.get('color', '')
#         price = var.get('price', '')
#         main_image = var.get('main_image', '')
#         sizes = var.get('sizes', [])
#         sizes_str = ','.join(sizes) if sizes else ''
#
#         # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –≤–∞—Ä–∏–∞—Ü–∏–∏
#         var_str = f"color:{color}; price:{price}; sizes:[{sizes_str}]; image:{main_image}"
#         parts.append(var_str)
#
#     return ' | '.join(parts)


def save_to_csv(products, filename='puma-products.csv'):
    """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ –∏ –∏—Ö –≤–∞—Ä–∏–∞—Ü–∏–π –≤ csv-—Ñ–æ—Ä–º–∞—Ç–µ, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å WooCommerce.
    """
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∏ —Ü–≤–µ—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
    all_colors = set()
    all_sizes = set()
    for product in products:
        for var in product.get('variations', []):
            all_colors.add(var.get('color', ''))
            all_sizes.update(var.get('sizes', []))

    # WooCommerce-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –ø–æ–ª—è + –≤–∞—à–∏
    fieldnames = [
        'ID', 'Type', 'SKU', 'Name', 'Parent', 'Brand', 'Product URL', 'Main Category', 'Subcategories',
        'Sale Price', 'Original Price', 'Description', 'Main Image', 'All Images',
        'attribute:Color', 'attribute:Size'
    ]

    with open(filename, mode='w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for product in products:
            # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–æ–≤–∞—Ä (variable)
            parent_id = product['random_id']
            writer.writerow({
                'ID': parent_id,
                'Type': 'variable',
                'SKU': parent_id,
                'Name': product['product_name'],
                'Parent': '',
                'Brand': product['brand'],
                'Product URL': product['product_url'],
                'Main Category': product['main_category'],
                'Subcategories': product['subcategories'],
                'Sale Price': product['sale_price'],
                'Original Price': product['original_price'],
                'Description': product['description'],
                'Main Image': '',
                'All Images': '',
                'attribute:Color': ', '.join(sorted(all_colors)),
                'attribute:Size': ', '.join(sorted(all_sizes)),
            })

            # –í–∞—Ä–∏–∞—Ü–∏–∏
            for var in product.get('variations', []):
                for size in var.get('sizes', []):
                    writer.writerow({
                        'ID': '',
                        'Type': 'variation',
                        'SKU': f"{parent_id}-{var.get('color', '')}-{size}",
                        'Name': '',
                        'Parent': parent_id,
                        'Brand': '',
                        'Product URL': product['product_url'],
                        'Main Category': '',
                        'Subcategories': '',
                        'Sale Price': product['sale_price'],
                        'Original Price': product['original_price'],
                        'Description': '',  # –û–±—ã—á–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ —É –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
                        'Main Image': var.get('main_image', ''),
                        'All Images': ','.join(var.get('all_images', [])),
                        'attribute:Color': var.get('color', ''),
                        'attribute:Size': size,
                    })

    print(f"CSV –¥–ª—è WooCommerce —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filename}")


def save_to_json(data, filename='products.json'):
    if not data:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")
        return
    with open(filename, 'w', encoding='utf-8') as jsonfile:
        json.dump(data, jsonfile, indent=4, ensure_ascii=False)
    print(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª {filename}")

def get_or_create_brand(brand_name):
    slug = slugify(brand_name)
    brand = Brand.objects.filter(slug=slug).first()
    if brand is None:
        brand = Brand.objects.create(name=brand_name, slug=slug)
    return brand

def get_or_create_category(name, parent=None):
    category = Category.objects.filter(name=name, parent=parent).first()
    if category is None:
        category = Category.objects.create(name=name, parent=parent)
    return category

# def save_image_from_url(instance, image_field_name, image_url):
#     try:
#         response = requests.get(image_url)
#         response.raise_for_status()
#         img_data = response.content
#         filename = image_url.split("/")[-1].split("?")[0]  # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ URL
#         getattr(instance, image_field_name).save(filename, ContentFile(img_data), save=True)
#         print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
#     except Exception as e:
#         print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_url}: {e}")



MAX_LENGTH = 100  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è CharField, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ–Ω—è–π—Ç–µ

def truncate_str(s, max_len=MAX_LENGTH):
    if not s:
        return ''
    return s[:max_len] if len(s) > max_len else s



def save_image_from_url(instance, field_name, url):
    try:
        if not url:
            return
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        file_name = os.path.basename(url.split("?")[0])
        getattr(instance, field_name).save(file_name, ContentFile(response.content), save=True)
        print(f"üì∏ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {file_name}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {url}: {e}")

def save_parsed_product_to_db(parsed_product, brand_name='Puma'):
    print("\nüîß –ù–∞—á–∏–Ω–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞...")

    # 1. –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
    parent = None
    main_cat, *subcats = [c.strip() for c in ([parsed_product.get('main_category')] + parsed_product.get('subcategories', '').split('/')) if c.strip()]
    for cat_name in [main_cat] + subcats:
        parent, _ = Category.objects.get_or_create(name=cat_name, parent=parent)
    category = parent
    print(f"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category.name}")

    # 2. –ë—Ä–µ–Ω–¥
    brand_slug = slugify(brand_name)
    brand = Brand.objects.filter(slug=brand_slug).first()
    if not brand:
        brand = Brand.objects.create(name=brand_name, slug=brand_slug)
        print(f"üÜï –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –±—Ä–µ–Ω–¥: {brand_name}")
    else:
        print(f"üîÑ –ù–∞–π–¥–µ–Ω –±—Ä–µ–Ω–¥: {brand_name}")

    # 3. –ü—Ä–æ–¥—É–∫—Ç
    base_slug = slugify(parsed_product['product_name'])
    product_slug = f"{base_slug}"[:500]
    print(f"üÜî Slug –ø—Ä–æ–¥—É–∫—Ç–∞: {product_slug}")

    try:
        product = Product.objects.get(slug=product_slug)
        print("üîÑ –ü—Ä–æ–¥—É–∫—Ç –Ω–∞–π–¥–µ–Ω, –æ–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ...")

        updated = False
        if product.name != parsed_product['product_name']:
            product.name = parsed_product['product_name']
            updated = True
        if product.desc != parsed_product.get('description', ''):
            product.desc = parsed_product.get('description', '')
            updated = True
        if product.brand != brand:
            product.brand = brand
            updated = True
        try:
            new_price = Decimal(str(parsed_product.get('sale_price', '')).replace('‚Çπ', '').replace(',',
                                                                                                   '').strip()) if parsed_product.get(
                'sale_price') else None
            if product.price != new_price:
                product.price = new_price
                updated = True
        except:
            pass
        try:
            new_discount = Decimal(str(parsed_product.get('original_price', '')).replace('‚Çπ', '').replace(',',
                                                                                                          '').strip()) if parsed_product.get(
                'original_price') else None
            if product.discount != new_discount:
                product.discount = new_discount
                updated = True
        except:
            pass
        if updated:
            product.save()
            print("üîÑ –ü—Ä–æ–¥—É–∫—Ç –æ–±–Ω–æ–≤–ª—ë–Ω")

    except Product.DoesNotExist:
        print("üÜï –ü—Ä–æ–¥—É–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π...")
        product = Product.objects.create(
            slug=product_slug,
            name=parsed_product['product_name'],
            desc=parsed_product.get('description', ''),
            brand=brand,
            price=None,
            discount=None
        )
        print("‚úÖ –ü—Ä–æ–¥—É–∫—Ç —Å–æ–∑–¥–∞–Ω")

    # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
    if not product.category.filter(id=category.id).exists():
        product.category.add(category)
        print(f"üìÅ –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–æ–±–∞–≤–ª–µ–Ω–∞: {category.name}")

    # –ì–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞ (–±–µ—Ä—ë–º –∏–∑ –ø–µ—Ä–≤–æ–π –≤–∞—Ä–∏–∞—Ü–∏–∏, –µ—Å–ª–∏ –Ω–µ—Ç)
    main_img_url = parsed_product.get('variations', [{}])[0].get('main_image')
    if main_img_url and (not product.image or not product.image.name):
        save_image_from_url(product, 'image', main_img_url)

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–æ—Ç–æ –≤–∞—Ä–∏–∞—Ü–∏–π –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ç–æ –ø—Ä–æ–¥—É–∫—Ç–∞
    all_variant_images = set()
    for var in parsed_product.get('variations', []):
        main_img = var.get('main_image')
        if main_img:
            all_variant_images.add(main_img)

    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ç–æ –ø—Ä–æ–¥—É–∫—Ç–∞ (–≤–∫–ª—é—á–∞—è —Ñ–æ—Ç–æ –≤–∞—Ä–∏–∞—Ü–∏–π)
    all_images = {img for var in parsed_product.get('variations', []) for img in var.get('all_images', [])}
    all_images.update(all_variant_images)  # –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Ñ–æ—Ç–æ –≤–∞—Ä–∏–∞—Ü–∏–π

    for img_url in all_images:
        if img_url:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É –ø—Ä–æ–¥—É–∫—Ç–∞
            exists = ProductImage.objects.filter(product=product, image=img_url).exists()
            if not exists:
                img_instance = ProductImage(product=product)
                save_image_from_url(img_instance, 'image', img_url)
                img_instance.save()

    # –í–∞—Ä–∏–∞—Ü–∏–∏ ‚Äî –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç –Ω–∞ —Ü–≤–µ—Ç —Å —Å–ø–∏—Å–∫–æ–º —Ä–∞–∑–º–µ—Ä–æ–≤
    for var in parsed_product.get('variations', []):
        color = (var.get('color') or '').strip()[:255]
        sizes = var.get('sizes', []) or ['']  # –µ—Å–ª–∏ –ø—É—Å—Ç–æ, —Å–æ–∑–¥–∞—ë–º –≤–∞—Ä–∏–∞—Ü–∏—é –±–µ–∑ —Ä–∞–∑–º–µ—Ä–∞
        main_image_url = var.get('main_image')
        try:
            price_var = Decimal(str(var.get('price')).replace('‚Çπ', '').replace(',', '').strip()) if var.get('price') else product.price
        except:
            price_var = product.price
        desc_var = var.get('description', '') or parsed_product.get('description', '')

        sku = f"{product_slug}-{color.replace(' ', '').replace('/', '')}"[:100]

        variant_obj, created = ProductVariant.objects.get_or_create(
            product=product,
            color=color,
            defaults={
                'size': sizes,
                'sku': sku,
                'price': price_var,
            }
        )
        if not created:
            updated = False
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã, –µ—Å–ª–∏ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
            if set(variant_obj.sizes) != set(sizes):
                variant_obj.sizes = sizes
                updated = True
            if variant_obj.price != price_var:
                variant_obj.price = price_var
                updated = True
            if variant_obj.description != desc_var:
                variant_obj.description = desc_var
                updated = True
            if updated:
                variant_obj.save()
                print(f"üîÑ –í–∞—Ä–∏–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞: color={color}")

        # –ì–ª–∞–≤–Ω–æ–µ —Ñ–æ—Ç–æ –≤–∞—Ä–∏–∞—Ü–∏–∏
        if main_image_url and (not variant_obj.image or not variant_obj.image.name):
            save_image_from_url(variant_obj, 'image', main_image_url)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ç–æ –≤–∞—Ä–∏–∞—Ü–∏–∏
        for img_url in var.get('all_images', []):
            if img_url and not variant_obj.additional_images.filter(image=img_url).exists():
                img_instance = VariantImage(variant=variant_obj)
                save_image_from_url(img_instance, 'image', img_url)
                img_instance.save()

        print(f"{'‚úÖ' if created else 'üîÑ'} –í–∞—Ä–∏–∞—Ü–∏—è: color={color}, sizes={sizes}, sku={sku}")

    print(f"\n‚úÖ –ü—Ä–æ–¥—É–∫—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {product.name}\n")


def collect_product_links_from_category(category_url):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
    """
    response = requests.get(category_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    product_links = set()
    # –ò—â–µ–º –≤—Å–µ <a> —Å –Ω—É–∂–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º
    for a in soup.find_all('a', href=True):
        href = a['href']
        if '/in/en/pd/' in href:
            full_url = 'https://in.puma.com' + href if href.startswith('/') else href
            product_links.add(full_url)
    return list(product_links)



# –ü—Ä–∏–º–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞:
if __name__ == "__main__":
    url = "https://in.puma.com/in/en/pd/court-shatter-low-sneakers/399844?size=0200&swatch=04"
    product_data = extract_product_info_and_variations(url)
    save_to_csv(product_data)
    save_to_json(product_data)
    for product in product_data:
        save_parsed_product_to_db(product)
# if __name__ == "__main__":
#     MAIN_CATEGORIES = [
#         'https://in.puma.com/in/en/rcb-launch',
#         # –î–æ–±–∞–≤—å—Ç–µ –¥—Ä—É–≥–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
#     ]
#
#     all_product_links = set()
#     for cat_url in MAIN_CATEGORIES:
#         links = collect_product_links_from_category(cat_url)
#         all_product_links.update(links)
#     print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_product_links)} —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")
#
#     all_products = []
#     for url in all_product_links:
#         print(f"–ü–∞—Ä—Å–∏–º: {url}")
#         products = extract_product_info_and_variations(url)
#         all_products.extend(products)  # –î–û–ë–ê–í–õ–Ø–ï–ú —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∞ –Ω–µ products.extend(products)
#
#     print(f"–í—Å–µ–≥–æ —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_products)}")
#
#     # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
#     save_to_csv(all_products)
#     save_to_json(all_products)
#     for product in all_products:
#         save_parsed_product_to_db(product)
